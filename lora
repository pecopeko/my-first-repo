import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from datasets import Dataset
from peft import LoraConfig, get_peft_model
from transformers import BitsAndBytesConfig

# JSONファイルの読み込み
with open("./data.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# データセットの作成
dataset = Dataset.from_dict({"text": [f"入力: {d['input']} 出力: {d['output']}" for d in data]})

# 既にダウンロード済みのローカルモデルを使用
model_name = "./mistralai/Mistral-7B-v0.1"

# トークナイザーの読み込み
tokenizer = AutoTokenizer.from_pretrained(model_name)

# ✅ `pad_token` の設定を追加
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token  # eos_token を pad_token として使用
    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # もし `[PAD]` を追加する場合はこちら

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
)

# モデルの読み込み（量子化適用）
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

# LoRA設定
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)

# トークナイズ関数
def tokenize_function(examples):
    texts = examples["text"]
    encoding = tokenizer(
        texts,
        truncation=True,
        max_length=128,
        padding="max_length"
    )
    encoding["labels"] = encoding["input_ids"].copy()  # ✅ labels を追加（エラー回避）
    return encoding

# データセットのトークン化
tokenized_dataset = dataset.map(tokenize_function, batched=True)

# 学習設定
training_args = TrainingArguments(
    output_dir="./lora_results",
    per_device_train_batch_size=4,
    num_train_epochs=3,
    learning_rate=1e-4,
    fp16=True,
    save_steps=100,
    logging_steps=50,
    save_total_limit=2,
    report_to="none"
)

# トレーナー設定
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
)

# 学習開始
trainer.train()

# LoRAアダプタの保存
model.save_pretrained("./lora_adapter")

print("LoRA の学習が完了しました！")
